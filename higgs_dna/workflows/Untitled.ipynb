{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47afa3b-9b7a-44e0-9091-4c12a98ee25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from higgs_dna.workflows.base import HggBaseProcessor\n",
    "from higgs_dna.tools.SC_eta import add_photon_SC_eta\n",
    "from higgs_dna.tools.EELeak_region import veto_EEleak_flag\n",
    "from higgs_dna.tools.EcalBadCalibCrystal_events import remove_EcalBadCalibCrystal_events\n",
    "from higgs_dna.selections.photon_selections_lowmass import photon_preselection_lowmass\n",
    "from higgs_dna.selections.lepton_selections import select_electrons, select_muons\n",
    "from higgs_dna.selections.jet_selections import select_jets, jetvetomap\n",
    "from higgs_dna.selections.lumi_selections import select_lumis\n",
    "from higgs_dna.selections.diphoton_selections import build_diphoton_candidates\n",
    "from higgs_dna.utils.dumping_utils import (\n",
    "    diphoton_ak_array,\n",
    "    dump_ak_array,\n",
    "    diphoton_list_to_pandas,\n",
    "    dump_pandas,\n",
    "    get_obj_syst_dict,\n",
    "    dress_branches,\n",
    ")\n",
    "from higgs_dna.utils.misc_utils import choose_jet\n",
    "from higgs_dna.tools.flow_corrections import apply_flow_corrections_to_photons\n",
    "from higgs_dna.tools.mass_decorrelator import decorrelate_mass_resolution\n",
    "from higgs_dna.tools.diphoton_mva_lowmass import (\n",
    "    add_diphoton_mva_inputs_for_lowmass,\n",
    "    eval_diphoton_mva_for_lowmass,\n",
    ")\n",
    "from higgs_dna.tools.dykiller_lowmass import eval_dykiller_for_lowmass\n",
    "\n",
    "# from higgs_dna.utils.dumping_utils import diphoton_list_to_pandas, dump_pandas\n",
    "from higgs_dna.systematics import object_systematics as available_object_systematics\n",
    "from higgs_dna.systematics import object_corrections as available_object_corrections\n",
    "from higgs_dna.systematics import weight_systematics as available_weight_systematics\n",
    "from higgs_dna.systematics import weight_corrections as available_weight_corrections\n",
    "from higgs_dna.systematics import apply_systematic_variations_object_level\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional\n",
    "import awkward as ak\n",
    "import numpy\n",
    "import sys\n",
    "import vector\n",
    "from coffea.analysis_tools import Weights\n",
    "from copy import deepcopy\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "\n",
    "def get_fiducial_mask(diphotons, fiducial_cut):\n",
    "    if fiducial_cut == \"classical\":\n",
    "        fid_det_passed = (\n",
    "            (diphotons.pho_lead.pt / diphotons.mass > 1 / 3)\n",
    "            & (diphotons.pho_sublead.pt / diphotons.mass > 1 / 4)\n",
    "            & (diphotons.pho_lead.pfRelIso03_all_quadratic * diphotons.pho_lead.pt < 10)\n",
    "            & (\n",
    "                (\n",
    "                    diphotons.pho_sublead.pfRelIso03_all_quadratic\n",
    "                    * diphotons.pho_sublead.pt\n",
    "                )\n",
    "                < 10\n",
    "            )\n",
    "            & (numpy.abs(diphotons.pho_lead.eta) < 2.5)\n",
    "            & (numpy.abs(diphotons.pho_sublead.eta) < 2.5)\n",
    "        )\n",
    "    elif fiducial_cut == \"geometric\":\n",
    "        fid_det_passed = (\n",
    "            (\n",
    "                numpy.sqrt(diphotons.pho_lead.pt * diphotons.pho_sublead.pt)\n",
    "                / diphotons.mass\n",
    "                > 1 / 3\n",
    "            )\n",
    "            & (diphotons.pho_sublead.pt / diphotons.mass > 1 / 4)\n",
    "            & (diphotons.pho_lead.pfRelIso03_all_quadratic * diphotons.pho_lead.pt < 10)\n",
    "            & (\n",
    "                diphotons.pho_sublead.pfRelIso03_all_quadratic\n",
    "                * diphotons.pho_sublead.pt\n",
    "                < 10\n",
    "            )\n",
    "            & (numpy.abs(diphotons.pho_lead.eta) < 2.5)\n",
    "            & (numpy.abs(diphotons.pho_sublead.eta) < 2.5)\n",
    "        )\n",
    "    elif fiducial_cut == \"none\":\n",
    "        fid_det_passed = (\n",
    "            diphotons.pho_lead.pt > -10\n",
    "        )  # This is a very dummy way but I do not know how to make a true array of outer shape of diphotons\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"You chose %s the fiducialCuts mode, but this is currently not supported. You should check your settings. For this run, no fiducial selection at detector level is applied.\"\n",
    "            % fiducial_cut\n",
    "        )\n",
    "        fid_det_passed = diphotons.pho_lead.pt > -10\n",
    "\n",
    "    return fid_det_passed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class lowmassVHProcessor(HggBaseProcessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metaconditions: Dict[str, Any],\n",
    "        systematics: Dict[str, List[Any]] = None,\n",
    "        corrections: Dict[str, List[Any]] = None,\n",
    "        apply_trigger: bool = False,\n",
    "        nano_version: int = None,\n",
    "        bTagEffFileName: Optional[str] = None,\n",
    "        output_location: Optional[str] = None,\n",
    "        taggers: Optional[List[Any]] = None,\n",
    "        trigger_group: str = \".*DoubleEG.*\",\n",
    "        analysis: str = \"lowMassAnalysis\",\n",
    "        applyCQR: bool = False,\n",
    "        skipJetVetoMap: bool = False,\n",
    "        year: Dict[str, List[str]] = None,\n",
    "        fiducialCuts: str = \"none\",\n",
    "        doDeco: bool = False,\n",
    "        Smear_sigma_m: bool = False,\n",
    "        doFlow_corrections: bool = False,\n",
    "        output_format: str = \"parquet\",\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            metaconditions,\n",
    "            systematics=systematics,\n",
    "            corrections=corrections,\n",
    "            apply_trigger=apply_trigger,\n",
    "            nano_version=nano_version,\n",
    "            bTagEffFileName=bTagEffFileName,\n",
    "            output_location=output_location,\n",
    "            taggers=taggers,\n",
    "            trigger_group=trigger_group,\n",
    "            analysis=analysis,\n",
    "            applyCQR=applyCQR,\n",
    "            skipJetVetoMap=skipJetVetoMap,\n",
    "            year=year,\n",
    "            fiducialCuts=fiducialCuts,\n",
    "            doDeco=doDeco,\n",
    "            Smear_sigma_m=Smear_sigma_m,\n",
    "            doFlow_corrections=doFlow_corrections,\n",
    "            output_format=output_format,\n",
    "        )\n",
    "\n",
    "        self.nano_version = nano_version\n",
    "\n",
    "        # diphoton preselection cuts\n",
    "        self.min_pt_photon = 18.0\n",
    "        self.min_pt_lead_photon = 30.0\n",
    "        self.e_veto = \"presel\"  # presel/single_invert/double_invert\n",
    "        self.trigger_group = \".*DoubleEG.*\"\n",
    "        self.analysis = \"lowMassAnalysis\"\n",
    "\n",
    "    def process_extra(self, events: ak.Array) -> ak.Array:\n",
    "        return events, {}\n",
    "\n",
    "    def apply_filters(self, events: ak.Array) -> ak.Array:\n",
    "        # met filters\n",
    "        met_filters = self.meta[\"flashggMetFilters\"][self.data_kind]\n",
    "        filtered = functools.reduce(\n",
    "            operator.and_,\n",
    "            (events.Flag[metfilter.split(\"_\")[-1]] for metfilter in met_filters),\n",
    "        )\n",
    "\n",
    "        return events[filtered]\n",
    "\n",
    "    def apply_triggers(\n",
    "        self, events: ak.Array, apply_to_mc: bool = False\n",
    "    ) -> ak.Array:\n",
    "        # trigger selection\n",
    "        logger.debug(\n",
    "            f\"[apply_triggers] {self.trigger_group} {self.analysis} {self.data_kind} {apply_to_mc}\"\n",
    "        )\n",
    "        triggered = ak.ones_like(events.event)\n",
    "\n",
    "        if self.apply_trigger:\n",
    "            if not apply_to_mc and self.data_kind == \"mc\":\n",
    "                return events\n",
    "            else:\n",
    "                trigger_names = []\n",
    "                triggers = self.meta[\"TriggerPaths\"][self.trigger_group][self.analysis]\n",
    "                hlt = events.HLT\n",
    "                for trigger in triggers:\n",
    "                    actual_trigger = trigger.replace(\"HLT_\", \"\").replace(\"*\", \"\")\n",
    "                    for field in hlt.fields:\n",
    "                        if field.startswith(actual_trigger):\n",
    "                            trigger_names.append(field)\n",
    "                triggered = functools.reduce(\n",
    "                    operator.or_, (hlt[trigger_name] for trigger_name in trigger_names)\n",
    "                )\n",
    "                return events[triggered]\n",
    "\n",
    "        return events\n",
    "\n",
    "    def process(self, events: ak.Array) -> Dict[Any, Any]:\n",
    "        dataset_name = events.metadata[\"dataset\"]\n",
    "\n",
    "        # data or monte carlo?\n",
    "        self.data_kind = \"mc\" if hasattr(events, \"GenPart\") else \"data\"\n",
    "\n",
    "        # here we start recording possible coffea accumulators\n",
    "        # most likely histograms, could be counters, arrays, ...\n",
    "        histos_etc = {}\n",
    "        histos_etc[dataset_name] = {}\n",
    "        if self.data_kind == \"mc\":\n",
    "            histos_etc[dataset_name][\"nTot\"] = int(\n",
    "                ak.num(events.genWeight, axis=0)\n",
    "            )\n",
    "            histos_etc[dataset_name][\"nPos\"] = int(ak.sum(events.genWeight > 0))\n",
    "            histos_etc[dataset_name][\"nNeg\"] = int(ak.sum(events.genWeight < 0))\n",
    "            histos_etc[dataset_name][\"nEff\"] = int(\n",
    "                histos_etc[dataset_name][\"nPos\"] - histos_etc[dataset_name][\"nNeg\"]\n",
    "            )\n",
    "            histos_etc[dataset_name][\"genWeightSum\"] = float(\n",
    "                ak.sum(events.genWeight)\n",
    "            )\n",
    "        else:\n",
    "            histos_etc[dataset_name][\"nTot\"] = int(len(events))\n",
    "            histos_etc[dataset_name][\"nPos\"] = int(histos_etc[dataset_name][\"nTot\"])\n",
    "            histos_etc[dataset_name][\"nNeg\"] = int(0)\n",
    "            histos_etc[dataset_name][\"nEff\"] = int(histos_etc[dataset_name][\"nTot\"])\n",
    "            histos_etc[dataset_name][\"genWeightSum\"] = float(len(events))\n",
    "\n",
    "        # lumi mask\n",
    "        if self.data_kind == \"data\":\n",
    "            try:\n",
    "                lumimask = select_lumis(self.year[dataset_name][0], events, logger)\n",
    "                events = events[lumimask]\n",
    "            except:\n",
    "                logger.info(\n",
    "                    f\"[ lumimask ] Skip now! Unable to find year info of {dataset_name}\"\n",
    "                )\n",
    "        # apply jetvetomap\n",
    "        if not self.skipJetVetoMap:\n",
    "            events = jetvetomap(\n",
    "                self, events, logger, dataset_name, year=self.year[dataset_name][0]\n",
    "            )\n",
    "        # metadata array to append to higgsdna output\n",
    "        metadata = {}\n",
    "\n",
    "        if self.data_kind == \"mc\":\n",
    "            # Add sum of gen weights before selection for normalisation in postprocessing\n",
    "            metadata[\"sum_genw_presel\"] = str(ak.sum(events.genWeight))\n",
    "        else:\n",
    "            metadata[\"sum_genw_presel\"] = \"Data\"\n",
    "\n",
    "        # apply filters and triggers\n",
    "        events = self.apply_filters(events)\n",
    "        # ! by default, trigger is not applied to MC\n",
    "        # ! if need to be applied, change apply_to_mc to True\n",
    "        events = self.apply_triggers(events, apply_to_mc=False)\n",
    "\n",
    "        # remove events affected by EcalBadCalibCrystal\n",
    "        if self.data_kind == \"data\":\n",
    "            events = remove_EcalBadCalibCrystal_events(events)\n",
    "\n",
    "        # we need ScEta for corrections and systematics, which is not present in NanoAODv11 but can be calculated using PV\n",
    "        events.Photon = add_photon_SC_eta(events.Photon, events.PV)\n",
    "\n",
    "        # add veto EE leak branch for photons, could also be used for electrons\n",
    "        if (\n",
    "            self.year[dataset_name][0] == \"2022EE\"\n",
    "            or self.year[dataset_name][0] == \"2022postEE\"\n",
    "        ):\n",
    "            events.Photon = veto_EEleak_flag(self, events.Photon)\n",
    "\n",
    "        # read which systematics and corrections to process\n",
    "        try:\n",
    "            correction_names = self.corrections[dataset_name]\n",
    "        except KeyError:\n",
    "            correction_names = []\n",
    "        try:\n",
    "            systematic_names = self.systematics[dataset_name]\n",
    "        except KeyError:\n",
    "            systematic_names = []\n",
    "\n",
    "        # If --Smear-sigma_m == True and no Smearing correction in .json for MC throws an error, since the pt scpectrum need to be smeared in order to properly calculate the smeared sigma_m_m\n",
    "        if (\n",
    "            self.data_kind == \"mc\"\n",
    "            and self.Smear_sigma_m\n",
    "            and (\"Smearing_Trad\" not in correction_names and \"Smearing_IJazZ\" not in correction_names and \"Smearing2G_IJazZ\" not in correction_names)\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Smearing_Trad or  Smearing_IJazZ or Smearing2G_IJazZ should be specified in the corrections field in .json in order to smear the mass!\"\n",
    "            )\n",
    "            sys.exit(0)\n",
    "\n",
    "        # save raw pt if we use scale/smearing corrections\n",
    "        # These needs to be before the smearing of the mass resolution in order to have the raw pt for the function\n",
    "        s_or_s_applied = False\n",
    "        for correction in correction_names:\n",
    "            logger.info(\"There is a correction: \" + correction)\n",
    "            if \"scale\" or \"smearing\" in correction.lower():\n",
    "                s_or_s_applied = True\n",
    "        if s_or_s_applied:\n",
    "            events.Photon[\"pt_raw\"] = ak.copy(events.Photon.pt)\n",
    "\n",
    "        # Since now we are applying Smearing term to the sigma_m_over_m i added this portion of code\n",
    "        # specially for the estimation of smearing terms for the data events [data pt/energy] are not smeared!\n",
    "        if self.data_kind == \"data\" and self.Smear_sigma_m:\n",
    "            if \"Scale_Trad\" in correction_names:\n",
    "                correction_name = \"Smearing_Trad\"\n",
    "            elif \"Scale_IJazZ\" in correction_names:\n",
    "                correction_name = \"Smearing_IJazZ\"\n",
    "            elif \"Scale2G_IJazZ\" in correction_names:\n",
    "                correction_name = \"Smearing2G_IJazZ\"\n",
    "            else:\n",
    "                logger.info('Specify a scale correction for the data in the corrections field in .json in order to smear the mass!')\n",
    "                sys.exit(0)\n",
    "\n",
    "            logger.info(\n",
    "                f\"\\nApplying correction {correction_name} to dataset {dataset_name}\\n\"\n",
    "            )\n",
    "            varying_function = available_object_corrections[correction_name]\n",
    "            events = varying_function(events=events, year=self.year[dataset_name][0])\n",
    "\n",
    "        for correction_name in correction_names:\n",
    "            if correction_name in available_object_corrections.keys():\n",
    "                logger.info(\n",
    "                    f\"Applying correction {correction_name} to dataset {dataset_name}\"\n",
    "                )\n",
    "                varying_function = available_object_corrections[correction_name]\n",
    "                events = varying_function(\n",
    "                    events=events, year=self.year[dataset_name][0]\n",
    "                )\n",
    "            elif correction_name in available_weight_corrections:\n",
    "                # event weight corrections will be applied after photon preselection / application of further taggers\n",
    "                continue\n",
    "            else:\n",
    "                # may want to throw an error instead, needs to be discussed\n",
    "                warnings.warn(f\"Could not process correction {correction_name}.\")\n",
    "                continue\n",
    "\n",
    "        original_photons = events.Photon\n",
    "        # NOTE: jet jerc systematics are added in the correction functions and handled later\n",
    "        original_jets = events.Jet\n",
    "        original_muons = events.Muon                                    #### CHMA\n",
    "        original_electrons = events.Electron                            #### CHMA\n",
    "\n",
    "        # Computing the normalizing flow correction\n",
    "        if self.data_kind == \"mc\" and self.doFlow_corrections:\n",
    "            original_photons = apply_flow_corrections_to_photons(\n",
    "                original_photons,\n",
    "                events,\n",
    "                self.meta,\n",
    "                self.year[dataset_name][0],\n",
    "                self.add_photonid_mva_run3,\n",
    "                logger\n",
    "            )\n",
    "\n",
    "        # Add additional collections if object systematics should be applied\n",
    "        collections = {\n",
    "            \"Photon\": original_photons,\n",
    "            \"Jet\": original_jets,\n",
    "            \"Muon\": original_muons,\n",
    "            \"Electron\": original_electrons\n",
    "        }\n",
    "\n",
    "        # Apply the systematic variations.\n",
    "        collections = apply_systematic_variations_object_level(\n",
    "            systematic_names,\n",
    "            events,\n",
    "            self.year[dataset_name][0],\n",
    "            logger,\n",
    "            available_object_systematics,\n",
    "            available_weight_systematics,\n",
    "            collections\n",
    "        )\n",
    "\n",
    "        original_photons = collections[\"Photon\"]\n",
    "        original_jets = collections[\"Jet\"]\n",
    "        original_muons = collections[\"Muon\"]\n",
    "        original_electrons = collections[\"Electron\"]\n",
    "\n",
    "        # Writing systematic variations to dicts\n",
    "        photons_dct = {}\n",
    "        photons_dct[\"nominal\"] = original_photons\n",
    "        logger.debug(original_photons.systematics.fields)\n",
    "        for systematic in original_photons.systematics.fields:\n",
    "            for variation in original_photons.systematics[systematic].fields:\n",
    "                # deepcopy to allow for independent calculations on photon variables with CQR\n",
    "                photons_dct[f\"{systematic}_{variation}\"] = deepcopy(\n",
    "                    original_photons.systematics[systematic][variation]\n",
    "                )\n",
    "\n",
    "        electrons_dct = {}\n",
    "        electrons_dct[\"nominal\"] = original_electrons\n",
    "        logger.debug(original_electrons.systematics.fields)\n",
    "        for systematic in original_electrons.systematics.fields:\n",
    "            for variation in original_electrons.systematics[systematic].fields:\n",
    "                # no deepcopy here unless we find a case where it's actually needed\n",
    "                electrons_dct[f\"{systematic}_{variation}\"] = original_electrons.systematics[systematic][variation]\n",
    "        muons_dct = {}\n",
    "        muons_dct[\"nominal\"] = original_muons\n",
    "        logger.debug(original_muons.systematics.fields)\n",
    "        for systematic in original_muons.systematics.fields:\n",
    "            for variation in original_muons.systematics[systematic].fields:\n",
    "                # no deepcopy here unless we find a case where it's actually needed\n",
    "                muons_dct[f\"{systematic}_{variation}\"] = original_muons.systematics[systematic][variation]\n",
    "\n",
    "\n",
    "        # NOTE: jet jerc systematics are added in the corrections, now extract those variations and create the dictionary\n",
    "        jerc_syst_list, jets_dct = get_obj_syst_dict(original_jets, [\"pt\", \"mass\"])\n",
    "        # object systematics dictionary\n",
    "        logger.debug(f\"[ jerc systematics ] {jerc_syst_list}\")\n",
    "\n",
    "        # Build the flattened array of all possible variations\n",
    "        variations_combined = []\n",
    "        variations_combined.append(original_photons.systematics.fields)\n",
    "        variations_combined.append(original_electrons.systematics.fields)\n",
    "        variations_combined.append(original_muons.systematics.fields)\n",
    "        # NOTE: jet jerc systematics are not added with add_systematics\n",
    "        variations_combined.append(jerc_syst_list)\n",
    "        # Flatten\n",
    "        variations_flattened = sum(\n",
    "            variations_combined, []\n",
    "        )  # Begin with empty list and keep concatenating\n",
    "        # Attach _down and _up\n",
    "        variations = [\n",
    "            item + suffix\n",
    "            for item in variations_flattened\n",
    "            for suffix in [\"_down\", \"_up\"]\n",
    "        ]\n",
    "        # Add nominal to the list\n",
    "        variations.append(\"nominal\")\n",
    "        logger.debug(f\"[systematics variations] {variations}\")\n",
    "\n",
    "        for variation in variations:\n",
    "            photons, electrons, muons, jets = photons_dct[\"nominal\"], electrons_dct[\"nominal\"], muons_dct[\"nominal\"], events.Jet\n",
    "            if variation == \"nominal\":\n",
    "                pass  # Do nothing since we already get the unvaried, but nominally corrected objets above\n",
    "            elif variation in [*photons_dct]:  # [*dict] gets the keys of the dict since Python >= 3.5\n",
    "                photons = photons_dct[variation]\n",
    "            elif variation in [*electrons_dct]:\n",
    "                electrons = electrons_dct[variation]\n",
    "            elif variation in [*muons_dct]:\n",
    "                muons = muons_dct[variation]\n",
    "            elif variation in [*jets_dct]:\n",
    "                jets = jets_dct[variation]\n",
    "            do_variation = (\n",
    "                variation  # We can also simplify this a bit but for now it works\n",
    "            )\n",
    "\n",
    "            if self.chained_quantile is not None:\n",
    "                photons = self.chained_quantile.apply(photons, events)\n",
    "            # recompute photonid_mva on the fly\n",
    "            if self.photonid_mva_EB and self.photonid_mva_EE:\n",
    "                photons = self.add_photonid_mva(photons, events)\n",
    "\n",
    "            # photon preselection\n",
    "            photons = photon_preselection_lowmass(\n",
    "                self, photons, events, year=self.year[dataset_name][0]\n",
    "            )\n",
    "\n",
    "            diphotons = build_diphoton_candidates(photons, self.min_pt_lead_photon)\n",
    "\n",
    "            # ! only keep both pass/single pass/none pass pairs\n",
    "            # presel: both photons don't have pixelSeed\n",
    "            # single_invert: one photon has pixelSeed, another doesn't have pixelSeed\n",
    "            # double_invert: both photons have pixelSeed\n",
    "            if self.e_veto == \"presel\":\n",
    "                diphotons = diphotons[\n",
    "                    (diphotons[\"pho_lead\"].pixelSeed < 0.5)\n",
    "                    & (diphotons[\"pho_sublead\"].pixelSeed < 0.5)\n",
    "                ]\n",
    "            elif self.e_veto == \"single_invert\":\n",
    "                diphotons = diphotons[\n",
    "                    (\n",
    "                        (diphotons[\"pho_lead\"].pixelSeed > 0.5)\n",
    "                        & (diphotons[\"pho_sublead\"].pixelSeed < 0.5)\n",
    "                    )\n",
    "                    | (\n",
    "                        (diphotons[\"pho_lead\"].pixelSeed < 0.5)\n",
    "                        & (diphotons[\"pho_sublead\"].pixelSeed > 0.5)\n",
    "                    )\n",
    "                ]\n",
    "            elif self.e_veto == \"double_invert\":\n",
    "                diphotons = diphotons[\n",
    "                    (diphotons[\"pho_lead\"].pixelSeed > 0.5)\n",
    "                    & (diphotons[\"pho_sublead\"].pixelSeed > 0.5)\n",
    "                ]\n",
    "            else:\n",
    "                logger.error(\n",
    "                    f\"[lowmass processor] '{self.e_veto}' is not allowed, please use presel/single_invert/double_invert\"\n",
    "                )\n",
    "\n",
    "            # sort diphotons by pT\n",
    "            diphotons = diphotons[ak.argsort(diphotons.pt, ascending=False)]\n",
    "\n",
    "            # Determine if event passes fiducial Hgg cuts at detector-level\n",
    "            # fid_det_passed = get_fiducial_mask(diphotons, self.fiducialCuts)\n",
    "            # diphotons = diphotons[fid_det_passed]\n",
    "            #\n",
    "            # if self.data_kind == \"mc\":\n",
    "\n",
    "            #     # Add the fiducial flags for particle level\n",
    "            #     diphotons[\"fiducialClassicalFlag\"] = get_fiducial_flag(\n",
    "            #         events, flavour=\"Classical\"\n",
    "            #     )\n",
    "            #     diphotons[\"fiducialGeometricFlag\"] = get_fiducial_flag(\n",
    "            #         events, flavour=\"Geometric\"\n",
    "            #     )\n",
    "\n",
    "            #     diphotons[\"PTH\"], diphotons[\"YH\"] = get_higgs_gen_attributes(events)\n",
    "\n",
    "            # baseline modifications to diphotons\n",
    "            if self.diphoton_mva is not None:\n",
    "                diphotons = self.add_diphoton_mva(diphotons, events)\n",
    "\n",
    "            # workflow specific processing\n",
    "            events, process_extra = self.process_extra(events)\n",
    "            histos_etc.update(process_extra)\n",
    "\n",
    "            btagMVA_selection = {\n",
    "                \"deepJet\": {\"btagDeepFlavB\": jets.btagDeepFlavB},  # Always available\n",
    "                \"particleNet\": {\"btagPNetB\": jets.btagPNetB} if self.nano_version >= 12 else {},\n",
    "                \"robustParticleTransformer\": {\"btagRobustParTAK4B\": jets.btagRobustParTAK4B} if self.nano_version >= 12 else {},\n",
    "            }\n",
    "\n",
    "            # jet_variables\n",
    "            jets = ak.zip(\n",
    "                {\n",
    "                    \"pt\": jets.pt,\n",
    "                    \"eta\": jets.eta,\n",
    "                    \"phi\": jets.phi,\n",
    "                    \"mass\": jets.mass,\n",
    "                    \"charge\": ak.zeros_like(\n",
    "                        jets.pt\n",
    "                    ),  # added this because jet charge is not a property of photons in nanoAOD v11. We just need the charge to build jet collection.\n",
    "                    **btagMVA_selection.get(self.bjet_mva, {}),\n",
    "                    \"hFlav\": (\n",
    "                        jets.hadronFlavour\n",
    "                        if self.data_kind == \"mc\"\n",
    "                        else ak.zeros_like(jets.pt)\n",
    "                    ),\n",
    "                    \"btagDeepFlavB\": jets.btagDeepFlavB,\n",
    "                    \"btagDeepFlav_CvB\": jets.btagDeepFlavCvB,\n",
    "                    \"btagDeepFlav_CvL\": jets.btagDeepFlavCvL,\n",
    "                    \"btagDeepFlav_QG\": jets.btagDeepFlavQG,\n",
    "                    \"jetId\": jets.jetId,\n",
    "                    **(\n",
    "                        {\"neHEF\": jets.neHEF, \"neEmEF\": jets.neEmEF, \"chEmEF\": jets.chEmEF, \"muEF\": jets.muEF} if self.nano_version == 12 else {}\n",
    "                    ),\n",
    "                    **(\n",
    "                        {\"neHEF\": jets.neHEF, \"neEmEF\": jets.neEmEF, \"chMultiplicity\": jets.chMultiplicity, \"neMultiplicity\": jets.neMultiplicity, \"chEmEF\": jets.chEmEF, \"chHEF\": jets.chHEF, \"muEF\": jets.muEF} if self.nano_version == 13 else {}\n",
    "                    ),\n",
    "                    # \"nJet\": jets.nJet,\n",
    "                }\n",
    "            )\n",
    "            jets = ak.with_name(jets, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "            electrons = ak.zip(\n",
    "                {\n",
    "                    \"pt\": events.Electron.pt,\n",
    "                    \"eta\": events.Electron.eta,\n",
    "                    \"phi\": events.Electron.phi,\n",
    "                    \"mass\": events.Electron.mass,\n",
    "                    \"charge\": events.Electron.charge,\n",
    "                    \"cutBased\": events.Electron.cutBased,\n",
    "                    \"cutBased_HEEP\": events.Electron.cutBased_HEEP,\n",
    "                    \"dr03TkSumPt\": events.Electron.dr03TkSumPt,\n",
    "                    \"dr03TkSumPtHEEP\": events.Electron.dr03TkSumPtHEEP,\n",
    "                    \"mvaIso\": events.Electron.mvaIso,\n",
    "                    \"mvaIso_WP90\": events.Electron.mvaIso_WP90,\n",
    "                    \"mvaIso_WP80\": events.Electron.mvaIso_WP80,\n",
    "                    \"pfRelIso03_all\": events.Electron.pfRelIso03_all,\n",
    "                    \"pfRelIso03_chg\": events.Electron.pfRelIso03_chg,\n",
    "                    \"r9\": events.Electron.r9,\n",
    "                    # \"nElectron\": events.Electron.nElectron,\n",
    "                }\n",
    "            )\n",
    "            electrons = ak.with_name(electrons, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "            muons = ak.zip(\n",
    "                {\n",
    "                    \"pt\": events.Muon.pt,\n",
    "                    \"eta\": events.Muon.eta,\n",
    "                    \"phi\": events.Muon.phi,\n",
    "                    \"mass\": events.Muon.mass,\n",
    "                    \"charge\": events.Muon.charge,\n",
    "                    \"tightId\": events.Muon.tightId,\n",
    "                    \"mediumId\": events.Muon.mediumId,\n",
    "                    \"looseId\": events.Muon.looseId,\n",
    "                    \"isGlobal\": events.Muon.isGlobal,\n",
    "                    \"pfIsoId\": events.Muon.pfIsoId,\n",
    "                    \"isPFcand\": events.Muon.isPFcand,\n",
    "                    \"mediumPromptId\": events.Muon.mediumPromptId,\n",
    "                    \"mvaMuID\": events.Muon.mvaMuID,\n",
    "                    \"pfRelIso03_all\": events.Muon.pfRelIso03_all,\n",
    "                    \"pfRelIso03_chg\": events.Muon.pfRelIso03_chg,\n",
    "                    # \"nMuon\": events.Muon.nMuon,\n",
    "                }\n",
    "            )\n",
    "            muons = ak.with_name(muons, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "            # RawMET = ak.zip(\n",
    "            #     {\n",
    "            #         \"pt\": events.RawMET.pt,\n",
    "            #         \"phi\": events.RawMET.phi,\n",
    "            #         \"sumEt\": events.RawMET.sumEt,\n",
    "            #     }\n",
    "            # )\n",
    "            # RawMET = ak.with_name(RawMET, \"PtEtaPhiMCandidate\")\n",
    "            \n",
    "            PuppiMET = ak.zip(\n",
    "                {\n",
    "                    \"pt\": events.PuppiMET.pt,\n",
    "                    \"phi\": events.PuppiMET.phi,\n",
    "                    \"phiJERDown\": events.PuppiMET.phiJERDown,\n",
    "                    \"phiJERUp\": events.PuppiMET.phiJERUp,\n",
    "                    \"phiJESDown\": events.PuppiMET.phiJESDown,\n",
    "                    \"phiJESUp\": events.PuppiMET.phiJESUp,\n",
    "                    \"phiUnclusteredDown\": events.PuppiMET.phiUnclusteredDown,\n",
    "                    \"phiUnclusteredUp\": events.PuppiMET.phiUnclusteredUp,\n",
    "                    \"ptJERDown\": events.PuppiMET.ptJERDown,\n",
    "                    \"ptJERUp\": events.PuppiMET.ptJERUp,\n",
    "                    \"ptJESDown\": events.PuppiMET.ptJESDown,\n",
    "                    \"ptJESUp\": events.PuppiMET.ptJESUp,\n",
    "                    \"ptUnclusteredDown\": events.PuppiMET.ptUnclusteredDown,\n",
    "                    \"ptUnclusteredUp\": events.PuppiMET.ptUnclusteredUp,\n",
    "                    \"sumEt\": events.PuppiMET.sumEt,\n",
    "                }\n",
    "            )\n",
    "            PuppiMET = ak.with_name(PuppiMET, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "            # RawPuppiMET = ak.zip(\n",
    "            #     {\n",
    "            #         \"pt\": events.RawPuppiMET.pt,\n",
    "            #         \"phi\": events.RawPuppiMET.phi,\n",
    "            #         \"sumEt\": events.RawPuppiMET.sumEt,\n",
    "            #     }\n",
    "            # )\n",
    "            # RawPuppiMET = ak.with_name(RawPuppiMET, \"PtEtaPhiMCandidate\")\n",
    "\n",
    "            # lepton cleaning\n",
    "            electrons = electrons[select_electrons(self, electrons, diphotons)]\n",
    "            muons = muons[select_muons(self, muons, diphotons)]\n",
    "\n",
    "            # jet selection and pt ordering\n",
    "            jets = jets[select_jets(self, jets, diphotons, muons, electrons)]\n",
    "            jets = jets[ak.argsort(jets.pt, ascending=False)]\n",
    "\n",
    "            # ordering lepton \n",
    "            electrons = electrons[ak.argsort(electrons.pt, ascending=False)]\n",
    "            muons = muons[ak.argsort(muons.pt, ascending=False)]\n",
    "            \n",
    "            # adding selected leptons and jets to events to be used in ctagging SF calculation\n",
    "            events[\"sel_jets\"] = jets\n",
    "            events[\"sel_electrons\"] = electrons\n",
    "            events[\"sel_muons\"] = muons\n",
    "            \n",
    "            num_jets = 5\n",
    "            jet_properties = [\"pt\", \"eta\", \"phi\", \"mass\", \"charge\", \"btagPNetB\", \"btagDeepFlavB\", \"btagDeepFlav_CvB\", \"btagDeepFlav_CvL\",  \"btagDeepFlav_QG\", \"neHEF\", \"neEmEF\", \"muEF\", \"chEmEF\", \"jetId\"]  #\n",
    "            if self.nano_version == 13:\n",
    "                jet_properties = jet_properties + [\"chMultiplicity\" , \"neMultiplicity\", \"chHEF\"]\n",
    "            for i in range(num_jets):\n",
    "                for prop in jet_properties:\n",
    "                    key = f\"jet{i+1}_{prop}\"\n",
    "                    value = choose_jet(getattr(jets, prop), i, -999.0)\n",
    "                    # Store the value in the diphotons dictionary\n",
    "                    diphotons[key] = value\n",
    "                    \n",
    "            n_jets = ak.num(jets)    \n",
    "            Njets2p5 = ak.num(jets[(jets.pt > 30) & (numpy.abs(jets.eta) < 2.5)])\n",
    "\n",
    "            diphotons[\"n_jets\"] = n_jets\n",
    "            diphotons[\"Njets2p5\"] = Njets2p5\n",
    "\n",
    "            num_electrons = 5  # Number of electrons to select\n",
    "            # Annotate diphotons with selected electrons' properties\n",
    "            electron_properties = [\"pt\", \"eta\", \"phi\", \"mass\", \"charge\", \"cutBased\", \"cutBased_HEEP\", \"dr03TkSumPt\", \"dr03TkSumPtHEEP\", \"mvaIso\", \"mvaIso_WP90\", \"mvaIso_WP80\", \"pfRelIso03_all\", \"pfRelIso03_chg\", \"r9\"]\n",
    "            for i in range(num_electrons):\n",
    "                for prop in electron_properties:\n",
    "                    key = f\"electron{i+1}_{prop}\"\n",
    "                    # Retrieve the value using the choose_jet function (which can be used for leptons as well)\n",
    "                    value = choose_jet(getattr(electrons, prop), i, -999.0)\n",
    "                    # Store the value in the diphotons dictionary\n",
    "                    diphotons[key] = value\n",
    "\n",
    "            n_electrons = ak.num(electrons)\n",
    "            diphotons[\"n_electrons\"] = n_electrons \n",
    "\n",
    "            num_muons = 5  # Number of muons to select\n",
    "            # Annotate diphotons with selected muons' properties\n",
    "            muon_properties = [\"pt\", \"eta\", \"phi\", \"mass\", \"charge\", \"tightId\", \"mediumId\", \"looseId\", \"isGlobal\", \"pfIsoId\", \"isPFcand\", \"mediumPromptId\", \"mvaMuID\", \"pfRelIso03_all\", \"pfRelIso03_chg\"]\n",
    "            for i in range(num_muons):\n",
    "                for prop in muon_properties:\n",
    "                    key = f\"muon{i+1}_{prop}\"\n",
    "                    # Retrieve the value using the choose_jet function (which can be used for leptons as well)\n",
    "                    value = choose_jet(getattr(muons, prop), i, -999.0)\n",
    "                    # Store the value in the diphotons dictionary\n",
    "                    diphotons[key] = value\n",
    "\n",
    "            n_muons = ak.num(muons)\n",
    "            diphotons[\"n_muons\"] = n_muons \n",
    "\n",
    "            # Saving Met features\n",
    "            # diphotons[\"RawMET_pt\"] = events.RawMET.pt\n",
    "            # diphotons[\"RawMET_phi\"] = events.RawMET.phi\n",
    "            # diphotons[\"RawMET_sumEt\"] = events.RawMET.sumEt\n",
    "\n",
    "            diphotons[\"PuppiMET_pt\"] = events.PuppiMET.pt\n",
    "            diphotons[\"PuppiMET_phi\"] = events.PuppiMET.phi\n",
    "            diphotons[\"PuppiMET_phiJERDown\"] = events.PuppiMET.phiJERDown\n",
    "            diphotons[\"PuppiMET_phiJERUp\"] = events.PuppiMET.phiJERUp\n",
    "            diphotons[\"PuppiMET_phiJESDown\"] = events.PuppiMET.phiJESDown\n",
    "            diphotons[\"PuppiMET_phiJESUp\"] = events.PuppiMET.phiJESUp\n",
    "            diphotons[\"PuppiMET_phiUnclusteredDown\"] = events.PuppiMET.phiUnclusteredDown\n",
    "            diphotons[\"PuppiMET_phiUnclusteredUp\"] = events.PuppiMET.phiUnclusteredUp\n",
    "            diphotons[\"PuppiMET_ptJERDown\"] = events.PuppiMET.ptJERDown\n",
    "            diphotons[\"PuppiMET_ptJERUp\"] = events.PuppiMET.ptJERUp\n",
    "            diphotons[\"PuppiMET_ptJESDown\"] = events.PuppiMET.ptJESDown\n",
    "            diphotons[\"PuppiMET_ptJESUp\"] = events.PuppiMET.ptJESUp\n",
    "            diphotons[\"PuppiMET_ptUnclusteredDown\"] = events.PuppiMET.ptUnclusteredDown\n",
    "            diphotons[\"PuppiMET_ptUnclusteredUp\"] = events.PuppiMET.ptUnclusteredUp\n",
    "            diphotons[\"PuppiMET_sumEt\"] = events.PuppiMET.sumEt\n",
    "            \n",
    "            \n",
    "            # * add diphoton mva inputs\n",
    "            if self.data_kind == \"mc\" and self.doFlow_corrections:\n",
    "                diphotons = add_diphoton_mva_inputs_for_lowmass(\n",
    "                    diphotons, events, mc_flow_corrected=True\n",
    "                )\n",
    "            else:\n",
    "                diphotons = add_diphoton_mva_inputs_for_lowmass(\n",
    "                    diphotons, events, mc_flow_corrected=False\n",
    "                )\n",
    "\n",
    "            # run taggers on the events list with added diphotons\n",
    "            # the shape here is ensured to be broadcastable\n",
    "            for tagger in self.taggers:\n",
    "                (\n",
    "                    diphotons[\"_\".join([tagger.name, str(tagger.priority)])],\n",
    "                    tagger_extra,\n",
    "                ) = tagger(\n",
    "                    events, diphotons\n",
    "                )  # creates new column in diphotons - tagger priority, or 0, also return list of histrograms here?\n",
    "                histos_etc.update(tagger_extra)\n",
    "\n",
    "            # if there are taggers to run, arbitrate by them first\n",
    "            # Deal with order of tagger priorities\n",
    "            # Turn from diphoton jagged array to whether or not an event was selected\n",
    "            if len(self.taggers):\n",
    "                counts = ak.num(diphotons.pt, axis=1)\n",
    "                flat_tags = numpy.stack(\n",
    "                    (\n",
    "                        ak.flatten(\n",
    "                            diphotons[\"_\".join([tagger.name, str(tagger.priority)])]\n",
    "                        )\n",
    "                        for tagger in self.taggers\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                )\n",
    "                tags = ak.from_regular(\n",
    "                    ak.unflatten(flat_tags, counts), axis=2\n",
    "                )\n",
    "                winner = ak.min(tags[tags != 0], axis=2)\n",
    "                diphotons[\"best_tag\"] = winner\n",
    "\n",
    "                # lowest priority is most important (ascending sort)\n",
    "                # leave in order of diphoton pT in case of ties (stable sort)\n",
    "                sorted = ak.argsort(diphotons.best_tag, stable=True)\n",
    "                diphotons = diphotons[sorted]\n",
    "\n",
    "            diphotons = ak.firsts(diphotons)\n",
    "            # set diphotons as part of the event record\n",
    "            events[f\"diphotons_{do_variation}\"] = diphotons\n",
    "            # annotate diphotons with event information\n",
    "            diphotons[\"event\"] = events.event\n",
    "            diphotons[\"lumi\"] = events.luminosityBlock\n",
    "            diphotons[\"run\"] = events.run\n",
    "            # nPV just for validation of pileup reweighting\n",
    "            diphotons[\"nPV\"] = events.PV.npvs\n",
    "            diphotons[\"fixedGridRhoAll\"] = events.Rho.fixedGridRhoAll\n",
    "            diphotons = dress_branches(diphotons, events.PV, \"PV\")\n",
    "            diphotons = dress_branches(diphotons, events.Rho, \"Rho\")\n",
    "            # annotate diphotons with dZ information (difference between z position of GenVtx and PV) as required by flashggfinalfits\n",
    "            if self.data_kind == \"mc\":\n",
    "                diphotons[\"genWeight\"] = events.genWeight\n",
    "                diphotons[\"dZ\"] = events.GenVtx.z - events.PV.z\n",
    "                # Necessary for differential xsec measurements in final fits (\"truth\" variables)\n",
    "                diphotons[\"HTXS_Higgs_pt\"] = events.HTXS.Higgs_pt\n",
    "                diphotons[\"HTXS_Higgs_y\"] = events.HTXS.Higgs_y\n",
    "                diphotons[\"HTXS_njets30\"] = (\n",
    "                    events.HTXS.njets30\n",
    "                )  # Need to clarify if this variable is suitable, does it fulfill abs(eta_j) < 2.5? Probably not\n",
    "                # Preparation for HTXS measurements later, start with stage 0 to disentangle VH into WH and ZH for final fits\n",
    "                diphotons[\"HTXS_stage_0\"] = events.HTXS.stage_0\n",
    "            # Fill zeros for data because there is no GenVtx for data, obviously\n",
    "            else:\n",
    "                diphotons[\"dZ\"] = ak.zeros_like(events.PV.z)\n",
    "\n",
    "            # drop events without a preselected diphoton candidate\n",
    "            # drop events without a tag, if there are tags\n",
    "            if len(self.taggers):\n",
    "                selection_mask = ~(\n",
    "                    ak.is_none(diphotons) | ak.is_none(diphotons.best_tag)\n",
    "                )\n",
    "                diphotons = diphotons[selection_mask]\n",
    "            else:\n",
    "                selection_mask = ~ak.is_none(diphotons)\n",
    "                diphotons = diphotons[selection_mask]\n",
    "\n",
    "            # * evaluate diphoton mva and dykiller score\n",
    "            # * after all selection and\n",
    "            # * before map pho_lead and pho_sublead with self.prefixes\n",
    "            diphotons = eval_diphoton_mva_for_lowmass(\n",
    "                diphotons, year=self.year[dataset_name][0]\n",
    "            )\n",
    "            diphotons = eval_dykiller_for_lowmass(\n",
    "                diphotons, year=self.year[dataset_name][0]\n",
    "            )\n",
    "\n",
    "            # return if there is no surviving events\n",
    "            if len(diphotons) == 0:\n",
    "                logger.debug(\"No surviving events in this run, return now!\")\n",
    "                return histos_etc\n",
    "            if self.data_kind == \"mc\":\n",
    "                # initiate Weight container here, after selection, since event selection cannot easily be applied to weight container afterwards\n",
    "                event_weights = Weights(size=len(events[selection_mask]))\n",
    "                # set weights to generator weights\n",
    "                event_weights._weight = events[\"genWeight\"][selection_mask]\n",
    "\n",
    "                # corrections to event weights:\n",
    "                for correction_name in correction_names:\n",
    "                    if correction_name in available_weight_corrections:\n",
    "                        logger.info(\n",
    "                            f\"Adding correction {correction_name} to weight collection of dataset {dataset_name}\"\n",
    "                        )\n",
    "                        varying_function = available_weight_corrections[correction_name]\n",
    "                        event_weights = varying_function(\n",
    "                            events=events[selection_mask],\n",
    "                            photons=events[f\"diphotons_{do_variation}\"][selection_mask],\n",
    "                            muons=events[\"sel_muons\"][selection_mask],\n",
    "                            electrons=events[\"sel_electrons\"][selection_mask],\n",
    "                            weights=event_weights,\n",
    "                            dataset_name=dataset_name,\n",
    "                            year=self.year[dataset_name][0],\n",
    "                        )\n",
    "\n",
    "                # systematic variations of event weights go to nominal output dataframe:\n",
    "                if do_variation == \"nominal\":\n",
    "                    for systematic_name in systematic_names:\n",
    "                        if systematic_name in available_weight_systematics:\n",
    "                            logger.info(\n",
    "                                f\"Adding systematic {systematic_name} to weight collection of dataset {dataset_name}\"\n",
    "                            )\n",
    "                            if systematic_name == \"LHEScale\":\n",
    "                                if hasattr(events, \"LHEScaleWeight\"):\n",
    "                                    diphotons[\"nweight_LHEScale\"] = ak.num(\n",
    "                                        events.LHEScaleWeight[selection_mask],\n",
    "                                        axis=1,\n",
    "                                    )\n",
    "                                    diphotons[\"weight_LHEScale\"] = (\n",
    "                                        events.LHEScaleWeight[selection_mask]\n",
    "                                    )\n",
    "                                else:\n",
    "                                    logger.info(\n",
    "                                        f\"No {systematic_name} Weights in dataset {dataset_name}\"\n",
    "                                    )\n",
    "                            elif systematic_name == \"LHEPdf\":\n",
    "                                if hasattr(events, \"LHEPdfWeight\"):\n",
    "                                    # two AlphaS weights are removed\n",
    "                                    diphotons[\"nweight_LHEPdf\"] = (\n",
    "                                        ak.num(\n",
    "                                            events.LHEPdfWeight[selection_mask],\n",
    "                                            axis=1,\n",
    "                                        )\n",
    "                                        - 2\n",
    "                                    )\n",
    "                                    diphotons[\"weight_LHEPdf\"] = events.LHEPdfWeight[\n",
    "                                        selection_mask\n",
    "                                    ][:, :-2]\n",
    "                                else:\n",
    "                                    logger.info(\n",
    "                                        f\"No {systematic_name} Weights in dataset {dataset_name}\"\n",
    "                                    )\n",
    "                            else:\n",
    "                                varying_function = available_weight_systematics[\n",
    "                                    systematic_name\n",
    "                                ]\n",
    "                                event_weights = varying_function(\n",
    "                                    events=events[selection_mask],\n",
    "                                    photons=events[f\"diphotons_{do_variation}\"][\n",
    "                                        selection_mask\n",
    "                                    ],\n",
    "                                    weights=event_weights,\n",
    "                                    dataset_name=dataset_name,\n",
    "                                    year=self.year[dataset_name][0],\n",
    "                                )\n",
    "\n",
    "                diphotons[\"weight\"] = event_weights.weight()\n",
    "                diphotons[\"weight_central\"] = (\n",
    "                    event_weights.weight() / events[\"genWeight\"][selection_mask]\n",
    "                )\n",
    "                # Store variations with respect to central weight\n",
    "                if do_variation == \"nominal\":\n",
    "                    if len(event_weights.variations):\n",
    "                        logger.info(\n",
    "                            \"Adding systematic weight variations to nominal output file.\"\n",
    "                        )\n",
    "                    for modifier in event_weights.variations:\n",
    "                        diphotons[\"weight_\" + modifier] = event_weights.weight(\n",
    "                            modifier=modifier\n",
    "                        )\n",
    "\n",
    "            # Add weight variables (=1) for data for consistent datasets\n",
    "            else:\n",
    "                diphotons[\"weight_central\"] = ak.ones_like(diphotons[\"event\"])\n",
    "                diphotons[\"weight\"] = ak.ones_like(diphotons[\"event\"])\n",
    "\n",
    "            ### Add mass resolution uncertainty\n",
    "            # Note that pt*cosh(eta) is equal to the energy of a four vector\n",
    "            # Note that you need to call it slightly different than in the output of HiggsDNA as pho_lead -> lead is only done in dumping utils\n",
    "            if self.data_kind == \"mc\" and self.doFlow_corrections:\n",
    "                diphotons = get_mass_resolution_uncertainty(\n",
    "                    diphotons, mc_flow_corrected=True\n",
    "                )\n",
    "            else:\n",
    "                diphotons = get_mass_resolution_uncertainty(\n",
    "                    diphotons, mc_flow_corrected=False\n",
    "                )\n",
    "\n",
    "            # This is the mass SigmaM/M value including the smearing term from the Scale and smearing\n",
    "            # The implementation follows the flashGG implementation -> https://github.com/cms-analysis/flashgg/blob/4edea8897e2a4b0518dca76ba6c9909c20c40ae7/DataFormats/src/Photon.cc#L293\n",
    "            # adittional flashGG link when the smearing of the SigmaE/E smearing is called -> https://github.com/cms-analysis/flashgg/blob/4edea8897e2a4b0518dca76ba6c9909c20c40ae7/Systematics/plugins/PhotonSigEoverESmearingEGMTool.cc#L83C40-L83C45\n",
    "            # Just a reminder, the pt/energy of teh data is not smearing, but the smearing term is added to the data sigma_m_over_m\n",
    "            if self.Smear_sigma_m:\n",
    "                if self.doFlow_corrections and self.data_kind == \"mc\":\n",
    "                    # Adding the smeared BDT error to the ntuples!\n",
    "                    diphotons = get_mass_resolution_smearing(\n",
    "                        diphotons, mc_flow_corrected=True\n",
    "                    )\n",
    "                else:\n",
    "                    # Adding the smeared BDT error to the ntuples!\n",
    "                    diphotons = get_mass_resolution_smearing(\n",
    "                        diphotons, mc_flow_corrected=False\n",
    "                    )\n",
    "\n",
    "            # Decorrelating the mass resolution - Still need to supress the decorrelator noises\n",
    "            if self.doDeco:\n",
    "\n",
    "                # Decorrelate nominal sigma_m_over_m\n",
    "                diphotons[\"sigma_m_over_m_nominal_decorr\"] = (\n",
    "                    decorrelate_mass_resolution(\n",
    "                        diphotons, type=\"nominal\", year=self.year[dataset_name][0]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # decorrelate smeared nominal sigma_m_overm_m\n",
    "                if self.Smear_sigma_m:\n",
    "                    diphotons[\"sigma_m_over_m_smeared_decorr\"] = (\n",
    "                        decorrelate_mass_resolution(\n",
    "                            diphotons, type=\"smeared\", year=self.year[dataset_name][0]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # decorrelate flow corrected sigma_m_over_m\n",
    "                if self.doFlow_corrections:\n",
    "                    diphotons[\"sigma_m_over_m_corr_decorr\"] = (\n",
    "                        decorrelate_mass_resolution(\n",
    "                            diphotons, type=\"corr\", year=self.year[dataset_name][0]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # decorrelate flow corrected smeared sigma_m_over_m\n",
    "                if self.doFlow_corrections and self.Smear_sigma_m:\n",
    "                    diphotons[\"sigma_m_over_m_corr_smeared_decorr\"] = (\n",
    "                        decorrelate_mass_resolution(\n",
    "                            diphotons,\n",
    "                            type=\"corr_smeared\",\n",
    "                            year=self.year[dataset_name][0],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # Instead of the nominal sigma_m_over_m, we will use the smeared version of it -> (https://indico.cern.ch/event/1319585/#169-update-on-the-run-3-mass-r)\n",
    "                # else:\n",
    "                #    warnings.warn(\"Smeamering need to be applied in order to decorrelate the (Smeared) mass resolution. -- Exiting!\")\n",
    "                #    sys.exit(0)\n",
    "\n",
    "            if self.output_location is not None:\n",
    "                if self.output_format == \"root\":\n",
    "                    df = diphoton_list_to_pandas(self, diphotons)\n",
    "                else:\n",
    "                    akarr = diphoton_ak_array(self, diphotons)\n",
    "\n",
    "                    # Remove fixedGridRhoAll from photons to avoid having event-level info per photon\n",
    "                    akarr = akarr[\n",
    "                        [\n",
    "                            field\n",
    "                            for field in akarr.fields\n",
    "                            if \"lead_fixedGridRhoAll\" not in field\n",
    "                        ]\n",
    "                    ]\n",
    "\n",
    "                fname = (\n",
    "                    events.behavior[\"__events_factory__\"]._partition_key.replace(\n",
    "                        \"/\", \"_\"\n",
    "                    )\n",
    "                    + \".%s\" % self.output_format\n",
    "                )\n",
    "                subdirs = []\n",
    "                if \"dataset\" in events.metadata:\n",
    "                    subdirs.append(events.metadata[\"dataset\"])\n",
    "                subdirs.append(do_variation)\n",
    "                if self.output_format == \"root\":\n",
    "                    dump_pandas(self, df, fname, self.output_location, subdirs)\n",
    "                else:\n",
    "                    dump_ak_array(\n",
    "                        self, akarr, fname, self.output_location, metadata, subdirs\n",
    "                    )\n",
    "\n",
    "        return histos_etc\n",
    "\n",
    "    def postprocess(self, accumulant: Dict[Any, Any]) -> Any:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
